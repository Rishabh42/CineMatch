Inference service (1 page max): Briefly describe how you implemented the recommendation service and how you derive a ranking of movies from your model. Provide a pointer to your implementation (e.g. to GitLab or other services).

### Implementation of the recommendation service
#### Backend service
The requests to the 

### Ranking determination of the services

#### Dockerization of our inference service
We have placed the model file and the app driver code in the same container which is exposed over port 8082.

#### CI/CD pipeline



